"""
This is an abstract class which shall act as the base class for all the attacks.
It has few utilities which might be useful for the sub-classes.
The subclasses would implement `abstract` method with logic specific to the kind of attack.
The code in this section is inspired by:
1. https://github.com/Harry24k/adversarial-attacks-pytorch
2. Official PyTorch documentation for adversarial attacks
"""

import torch
import os
import json
import shutil
import numpy as np

from environment_setup import PROJECT_ROOT_DIR


class AbstractAttack:
    def __init__(self, model, save_folder=None, min_val=0, max_val=1):
        super(AbstractAttack, self).__init__()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = model
        self.min_val = min_val
        self.max_val = max_val
        self.image_location, self.json_location, self.orig_image_location = self._bootstrap(save_folder)

    def attack(self, *params, **kwargs):
        """
        Abstract method to be implemented by each of the subclasses
        """
        raise NotImplementedError

    def set_attack_mode(self, targeted=False):
        """
        Set the attack mode.

        Arguments:
            targeted (bool) : 'original' (DEFAULT)
                         'targeted' - Use input labels as targeted labels.
        """
        self._targeted = 1 - 2 * targeted  # If targeted -1 else 1
        self.is_targeted_attack = targeted

    def save_perturbed_images(self, pertrub_pred, orig_pred, perturbed_data, names, ground_truth, orig_data, task_type):
        """
        Utility method for saving the perturbed samples along with original and ground_truth label. The images are
        saved as numpy tensors. The corresponding ground truth, perturbed and unperturbed model prediction labels are
        saved as a json file. Both the json file and numpy image files have the same name as the original image name.
        :param task_type: Indicates if `reid` or `attr` task
        :param pertrub_pred: Prediction made on the perturbed samples
        :param orig_pred: Prediction made on unperturbed samples
        :param perturbed_data: Perturbed samples generated by the attack
        :param names: Input image identifier
        :param ground_truth: Associated ground truth label for the unperturbed input
        :param orig_data: The unperturbed ground truth data
        :return: None
        """
        assert orig_pred.shape == pertrub_pred.shape, "Shape mismatch please verify"
        assert orig_pred.size(0) == pertrub_pred.size(0) == perturbed_data.size(0), "Batch dimension mismatch"
        assert self.image_location is not None and self.json_location is not None, "Can not save since folder " \
                                                                                   "location not given!! "
        self.model.eval()
        # Perhaps we can switch to simply saving all the perturbed samples that are generated
        for data_idx in range(orig_pred.size(0)):
            original_pred = orig_pred[data_idx]
            perturbed_pred = pertrub_pred[data_idx]
            gt = ground_truth[data_idx]
            img_name = str(names[data_idx].item())
            perturbed_image = perturbed_data[data_idx].squeeze().detach().cpu().numpy().transpose(1, 2, 0)
            orig_image = orig_data[data_idx].squeeze().detach().cpu().numpy().transpose(1, 2, 0)
            np.save(os.path.join(self.image_location, img_name), perturbed_image)
            np.save(os.path.join(self.orig_image_location, img_name), orig_image)
            json_data = self.create_json_data(gt=gt, original_pred=original_pred, perturbed_pred=perturbed_pred, task_type=task_type)
            json.dump(json_data, open(os.path.join(self.json_location, img_name + '.json'), "w"))

    @staticmethod
    def create_json_data(gt, original_pred, perturbed_pred, task_type):
        if task_type == 'attr':
            original_pred = original_pred.ge(0).squeeze()
            perturbed_pred = perturbed_pred.ge(0).squeeze()
            json_data = {'orig': original_pred.cpu().numpy().tolist(),
                         'perturb': perturbed_pred.cpu().numpy().tolist(),
                         'gt': gt.cpu().numpy().tolist()}
        elif task_type == 'reid':
            json_data = {'orig': torch.argmax(original_pred).cpu().numpy().item(),
                         'perturb': torch.argmax(perturbed_pred).cpu().numpy().item(),
                         'gt': gt.cpu().numpy().item()}
        else:
            raise ValueError('Unsupported Task Type selected for save operation')
        return json_data

    def _bootstrap(self, save_folder):
        """
        Bootstrapping method for generating the save locations for the json and numpy files for the input
        :param save_folder: The parent folder to save samples at
        :return:
        """
        if save_folder is None:
            print("Generated perturbed samples will not be saved since no folder location provided")
            return None, None, None
        image_location = os.path.join(PROJECT_ROOT_DIR, save_folder, 'images')
        orig_image_location = os.path.join(PROJECT_ROOT_DIR, save_folder, 'orig_images')
        json_location = os.path.join(PROJECT_ROOT_DIR, save_folder, 'json')
        if os.path.exists(os.path.join(PROJECT_ROOT_DIR, save_folder)):
            print(f"cleaning up {save_folder} folder since it already exists!")
            # cleanup data from previous run else it might cause corruption of data
            shutil.rmtree(save_folder)
        if not os.path.exists(image_location):
            os.makedirs(image_location)
        if not os.path.exists(json_location):
            os.makedirs(json_location)
        if not os.path.exists(orig_image_location):
            os.makedirs(orig_image_location)
        return image_location, json_location, orig_image_location

    def generate_perturbed_image(self, *input, **kwargs):
        """
        A wrapper function over the `attack` method which comes in handy during adversarial training and other defence
        techniques. The wrapper function can be ignored but it helps in ensuring correct `mode` of the network during
        training and adversarial generation steps.
        :param input: The input images
        :param kwargs: Additional key-value arguments if needed
        :return: adversarial images
        """
        if kwargs.get('train', False):
            self.model.train()
        else:
            self.model.eval()
        # During Adversarial training, we might need this flag

        adversarial_images = self.attack(*input, **kwargs)
        return adversarial_images
